{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EkoWviCDPVuG",
        "_s9Shfh8PYNe",
        "xC2cDhlVT6vy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1"
      ],
      "metadata": {
        "id": "EkoWviCDPVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "#setting environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "\n",
        "#set hyperparams\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "num_episodes = 10000 #doubled the training\n",
        "\n",
        "#initialize Q table\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "\n",
        "Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
        "\n",
        "#run Q learning algorithm now\n",
        "for i in range(num_episodes):\n",
        "    #reset env\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        if np.random.uniform(0,1) < 0.5:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        else:\n",
        "            action = np.argmax(Q[state,:])\n",
        "\n",
        "        #take action and observe next state and reward\n",
        "\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "\n",
        "        # Update the Q-value of the (state, action) pair\n",
        "\n",
        "        Q[state,action] = Q[state,action] + alpha *(reward + gamma *np.max(Q[next_state,:])-Q[state,action])\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "\n",
        "    print(f\"Episode {i}: Total reward = {total_reward}\")\n",
        "\n",
        "\n",
        "#tesing\n",
        "num_test_episodes = 500 #increased from 100 to 500\n",
        "num_test_steps = 500 #increased from 100 to 500\n",
        "num_successes = 0\n",
        "\n",
        "\n",
        "for i in range(num_test_episodes):\n",
        "    state=env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "\n",
        "    while not done and steps < num_test_steps:\n",
        "        action = np.argmax(Q[state,:])\n",
        "\n",
        "        next_state,reward,done,info = env.step(action)\n",
        "\n",
        "        state=next_state\n",
        "        steps+=1\n",
        "\n",
        "    if state==15:\n",
        "        num_successes+=1\n",
        "\n",
        "print(\"Success rate:\", num_successes/num_test_episodes)"
      ],
      "metadata": {
        "id": "YkSjqa2bOcE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2"
      ],
      "metadata": {
        "id": "_s9Shfh8PYNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OHA2_XVK3ug"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "env.reset()\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "q = np.zeros((num_states,num_actions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "num_successes = 0\n",
        "epsilon = 0.5\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  s = env.reset()\n",
        "  steps = 0\n",
        "  done = False\n",
        "  total_Reward = 0\n",
        "\n",
        "  while not done:\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "      a = np.argmax(q[s,:])\n",
        "    else:\n",
        "      a = env.action_space.sample()\n",
        "\n",
        "    next_state,reward,done,info = env.step(a)\n",
        "\n",
        "    q[s,a] = q[s,a] + alpha*(reward+gamma*np.max(q[next_state,a]-q[s,a]))\n",
        "    s = next_state\n",
        "    total_Reward += reward\n",
        "\n",
        "    print(f\"Episode {i}: Total reward = {total_Reward}\")\n"
      ],
      "metadata": {
        "id": "LnBUkkiaLQ8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3"
      ],
      "metadata": {
        "id": "xC2cDhlVT6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = 5\n",
        "sink_node = 0\n",
        "cost_matrix = np.array([[0, 1, 2, 3, 4],\n",
        "                        [1, 0, 1, 2, 3],\n",
        "                        [2, 1, 0, 1, 2],\n",
        "                        [3, 2, 1, 0, 1],\n",
        "                        [4, 3, 2, 1, 0]])\n",
        "\n",
        "Q = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "learning_rate = 0.8\n",
        "discount_factor = 0.95\n",
        "num_episodes = 2000\n",
        "max_steps = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(0, num_nodes)\n",
        "    total_cost = 0\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        epsilon = 0.1\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.randint(0, num_nodes)\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])\n",
        "\n",
        "        cost = cost_matrix[state, action]\n",
        "\n",
        "        Q[state, action] = Q[state, action] + learning_rate * (\n",
        "                cost + discount_factor * np.min(Q[action, :]) - Q[state, action])\n",
        "\n",
        "        total_cost += cost\n",
        "        state = action\n",
        "\n",
        "        if state == sink_node:\n",
        "            break\n",
        "\n",
        "    print(\"Episode:\", episode, \"Total Cost:\", total_cost)\n",
        "\n",
        "num_eval_episodes = 10\n",
        "eval_costs = []\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    state = np.random.randint(0, num_nodes)\n",
        "    total_cost = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(Q[state, :])\n",
        "        cost = cost_matrix[state, action]\n",
        "        total_cost += cost\n",
        "        state = action\n",
        "\n",
        "        if state == sink_node:\n",
        "            break\n",
        "\n",
        "    eval_costs.append(total_cost)\n",
        "\n",
        "print(\"Average Evaluation Cost:\", np.mean(eval_costs))\n"
      ],
      "metadata": {
        "id": "DUJ4DHAkT1tG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
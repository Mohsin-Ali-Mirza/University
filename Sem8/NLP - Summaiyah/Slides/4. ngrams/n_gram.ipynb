{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxpFqwSRNipC",
        "outputId": "a86b8bbc-abd8-4150-90ea-4d3586633c47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l18jdSLVNdL2",
        "outputId": "3b163728-c1d0-44b6-deaf-e73e96877e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probability for Sentence 1: 0.016\n",
            "Bigram Probability for Sentence 2: 0.12\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = \"I am Sam. Sam I am. Sam I like. Sam I do like. do I like Sam.\"\n",
        "sentence1 = \"Sam I do I like.\"\n",
        "sentence2 = \"Sam I am.\"\n",
        "\n",
        "corpus_tokens = word_tokenize(corpus.lower())\n",
        "corpus_bigrams = list(bigrams(corpus_tokens))\n",
        "\n",
        "def calculate_bigram_probability(test_sentence, corpus_bigrams):\n",
        "    test_tokens = word_tokenize(test_sentence.lower())\n",
        "\n",
        "    probability = 1.0\n",
        "    for i in range(len(test_tokens) - 1):\n",
        "        bigram = (test_tokens[i], test_tokens[i + 1])\n",
        "        bigram_count = corpus_bigrams.count(bigram)\n",
        "        first_word_count = sum(1 for bg in corpus_bigrams if bg[0] == bigram[0])\n",
        "        if first_word_count > 0:\n",
        "            bigram_probability = bigram_count / first_word_count\n",
        "        else:\n",
        "            bigram_probability = 0.0\n",
        "        probability *= bigram_probability\n",
        "\n",
        "    return probability\n",
        "probability_sentence1 = calculate_bigram_probability(sentence1, corpus_bigrams)\n",
        "probability_sentence2 = calculate_bigram_probability(sentence2, corpus_bigrams)\n",
        "\n",
        "print(\"Bigram Probability for Sentence 1:\", probability_sentence1)\n",
        "print(\"Bigram Probability for Sentence 2:\", probability_sentence2)\n"
      ]
    }
  ]
}